{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "154b2d0e-f7ce-453b-b3b7-eda0666a9795",
   "metadata": {},
   "source": [
    "# Generate embeddings for Ray documents using Ray Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ec3b5b-8e0d-4bdf-96bc-1f69774df271",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14d3e6-3262-4dd4-b4e1-e7a23ba06fdb",
   "metadata": {
    "tags": []
   },
   "source": [
    "0. Create an Anyscale workspace and disable autoscaling by deleting the worker node types.\n",
    "\n",
    "1. Setup a Postgres service:\n",
    "\n",
    "```bash\n",
    "# path: kuberay-rag-demo/anyscale/\n",
    "bash setup-pgvector.sh\n",
    "\n",
    "# path: kuberay-rag-demo/anyscale/\n",
    "# Initialize the schema.\n",
    "sudo -u postgres psql -f vector-768.sql\n",
    "```\n",
    "\n",
    "2. Download the Ray documents by running:\n",
    "    \n",
    "```bash\n",
    "# Please download and unzip the `ray-assistant-data` directory into the NFS `/mnt/shared_storage` to allow\n",
    "# all Ray nodes in the cluster access to the Ray docs.\n",
    "wget -O - https://storage.googleapis.com/ray-docs-embedding-postgres-dump/ray_docs.tar.gz | tar -xz\n",
    "```\n",
    "\n",
    "3. Create a `.env` file in the same directory as this notebook to initialize the `DB_CONNECTION_STRING` variable for connecting to Postgres. You can use `DB_CONNECTION_STRING=\"dbname=postgres user=postgres host=localhost password=postgres\"` for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08502d8-e9a9-4a50-acd9-76f77b18ada6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import psycopg\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from functools import partial\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from ray.data import ActorPoolStrategy\n",
    "from pgvector.psycopg import register_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633996c3-45b4-4ac6-961d-56b0df9156c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load environment variables from .env file in the same directory.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a4e4e-a46c-4157-8986-397944b9ebc7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ray.init(runtime_env={\n",
    "    \"env_vars\": {\n",
    "        \"DB_CONNECTION_STRING\": os.environ[\"DB_CONNECTION_STRING\"],\n",
    "    },\n",
    "})\n",
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab7f73e-bdd1-4c87-93d0-92ece2344432",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a1ed16-0f0d-4be1-951a-21a8cde1bfe9",
   "metadata": {},
   "source": [
    "Before we can start building our RAG application, we need to first create our vector DB that will contain our processed data sources.\n",
    "\n",
    "<img width=\"1000\" src=\"https://images.ctfassets.net/xjan103pcp94/3q5HUANQ4kS0V23cgEP0JF/ef3b62c5bc5c5c11b734fd3b73f6ea28/image3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c503edd-963a-4ec3-9182-39f7afc44153",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77456cf7-fe2b-4884-bfc2-99a2b4ffba1a",
   "metadata": {},
   "source": [
    "Weâ€™re going to then load our docs contents into a [Ray Dataset](https://docs.ray.io/en/latest/data/data.html) so that we can perform operations at scale on them (ex. embed, index, etc.). With large data sources, models and application serving needs, scale is a day-1 priority for LLM applications. We want to build our applications in such a way that they can scale as our needs grow without us having to change our code later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34f0ff-d61d-496c-9b7c-aac894ae40cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path for Ray docs. Note that access to Ray docs is required not only by the Ray head but also by the worker nodes.\n",
    "# Therefore, we need to place the Ray docs into shared storage.\n",
    "ray_docs_path = Path(\"/mnt/shared_storage/ray-assistant-data/docs.ray.io/en/master/\")\n",
    "assert ray_docs_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba6b43b-ea82-4c21-a885-57178cec3b44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ray.data.from_items([{\"path\": path} for path in ray_docs_path.rglob(\"*.html\") if not path.is_dir()])\n",
    "print(f\"{ds.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9edff6-6dbf-4037-9675-ae05cd3eb7a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f13f0dc-7f7a-4132-93e4-dc69aab164a2",
   "metadata": {},
   "source": [
    "Now that we have a dataset of all the paths to the html files, we're going to develop some functions that can appropriately extract the content from these files. We want to do this in a generalized manner so that we can perform this extraction across all of our docs pages (and so you can use it for your own data sources). Our process is to first identify the sections in our html page and then extract the text in between them. We save all of this into a list of dictionaries that map the text within a section to a specific url with a section anchor id.\n",
    "\n",
    "<img width=\"800\" src=\"https://images.ctfassets.net/xjan103pcp94/1eFnKmG5xqPIFtPupZ327X/f6152723e18322b90aaa8be5d2d5a6e4/image5.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167e6ed0-a24c-4576-aae4-23921df74147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility functions to extract sections from a Ray document.\n",
    "def extract_sections(record):\n",
    "    with open(record[\"path\"], \"r\", encoding=\"utf-8\") as html_file:\n",
    "        soup = BeautifulSoup(html_file, \"html.parser\")\n",
    "    sections = soup.find_all(\"section\")\n",
    "    section_list = []\n",
    "    for section in sections:\n",
    "        section_id = section.get(\"id\")\n",
    "        section_text = extract_text_from_section(section)\n",
    "        if section_id:\n",
    "            uri = path_to_uri(path=record[\"path\"])\n",
    "            section_list.append({\"source\": f\"{uri}#{section_id}\", \"text\": section_text})\n",
    "    return section_list\n",
    "\n",
    "def extract_text_from_section(section):\n",
    "    texts = []\n",
    "    for elem in section.children:\n",
    "        if isinstance(elem, NavigableString):\n",
    "            if elem.strip():\n",
    "                texts.append(elem.strip())\n",
    "        elif elem.name == \"section\":\n",
    "            continue\n",
    "        else:\n",
    "            texts.append(elem.get_text().strip())\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "def path_to_uri(path, scheme=\"https://\", domain=\"docs.ray.io\"):\n",
    "    return scheme + domain + str(path).split(domain)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8033de-2508-4030-a9a1-d6c792d5542a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_ray_doc = Path(ray_docs_path, \"rllib/rllib-env.html\")\n",
    "sections = extract_sections({\"path\": sample_ray_doc})\n",
    "print(f\"Extract {len(sections)} sections from the doc {sample_ray_doc}.\")\n",
    "print(f\"1st section in the Ray doc:\\n{json.dumps(sections[0], indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4ed64-2dd8-43c2-9831-cdf9d6d8004c",
   "metadata": {},
   "source": [
    "We can apply this extraction process (extract_section) in parallel to all the file paths in our dataset with just one line using Ray Data's [flat_map](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.flat_map.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4739d56-ddfe-42e5-9113-3d83d737999a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract sections\n",
    "sections_ds = ds.flat_map(extract_sections)\n",
    "sections_ds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39641fc6-adb2-4a6a-a8dd-f0433db814ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "section_lengths = []\n",
    "for section in sections_ds.take_all():\n",
    "    section_lengths.append(len(section[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c144ef2-ffa5-49ed-9bb3-60a50223854c",
   "metadata": {},
   "source": [
    "## Chunk data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c103aa-9b34-4d7b-8b42-78737eb0888d",
   "metadata": {},
   "source": [
    "We now have a list of sections (with text and source of each section) but we shouldn't directly use this as context to our RAG application just yet. The text lengths of each section are all varied and many are quite large chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b07383c-b898-4f4a-b008-837a8bf83015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(section_lengths, marker='o')\n",
    "plt.title(\"Section lengths\")\n",
    "plt.ylabel(\"# chars\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa7564d-fce5-4807-8bf2-de9f93c6f994",
   "metadata": {},
   "source": [
    "If we were to use these large sections, then we'd be inserting a lot of noisy/unwanted context and because all LLMs have a maximum context length, we wouldn't be able to fit too much other relevant context. So instead, we're going to split the text within each section into smaller chunks. Intuitively, smaller chunks will encapsulate single/few concepts and will be less noisy compared to larger chunks. We're going to choose some typical text splitting values (ex. chunk_size=300) to create our chunks for now but we'll be experimenting with a wider range of values later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6337e6-07b2-459d-a666-45de3aa945c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text splitter\n",
    "chunk_size = 300\n",
    "chunk_overlap = 50\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5208da-7196-49b5-9943-c0c9f0ab13b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chunk a sample section\n",
    "sample_section = sections_ds.take(1)[0]\n",
    "chunks = text_splitter.create_documents(\n",
    "    texts=[sample_section[\"text\"]], \n",
    "    metadatas=[{\"source\": sample_section[\"source\"]}])\n",
    "print(f\"A sample chunk:\\n{chunks[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d38775f-0a4b-4aad-866d-1a3231b32df7",
   "metadata": {},
   "source": [
    "While chunking our dataset is relatively fast, letâ€™s wrap the chunking logic into a function so that we can apply the workload at scale so that chunking remains just as fast as our data sources grow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c15a38d-066b-4623-8448-fdd0ac5ddf4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_section(section, chunk_size, chunk_overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len)\n",
    "    chunks = text_splitter.create_documents(\n",
    "        texts=[section[\"text\"]], \n",
    "        metadatas=[{\"source\": section[\"source\"]}])\n",
    "    return [{\"text\": chunk.page_content, \"source\": chunk.metadata[\"source\"]} for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be23c62-3108-49be-994d-5aa033162c71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale chunking\n",
    "chunks_ds = sections_ds.flat_map(partial(\n",
    "    chunk_section, \n",
    "    chunk_size=chunk_size, \n",
    "    chunk_overlap=chunk_overlap))\n",
    "print(f\"{chunks_ds.count()} chunks\")\n",
    "chunks_ds.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c23b31-e7b3-4078-abf7-683f448f5b19",
   "metadata": {},
   "source": [
    "## Embed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb44e2b4-d48a-4f27-b9e4-99579917b18e",
   "metadata": {},
   "source": [
    "Now that we've created small chunks from our sections, we need a way to identify the most relevant ones for a given query. A very effective and quick method is to embed our data using a pretrained model and use the same model to embed the query. We can then compute the distance between all of the chunk embeddings and our query embedding to determine the top-k chunks. There are many different pretrained models to choose from to embed our data but the most popular ones can be discovered through [HuggingFace's Massive Text Embedding Benchmark (MTEB)](https://huggingface.co/spaces/mteb/leaderboard) leaderboard. These models were pretrained on very large text corpus through tasks such as next/masked token prediction which allowed them to learn to represent subtokens in N dimensions and capture semantic relationships. We can leverage this to represent our data and identify the most relevant contexts to use to answer a given query. We're using Langchain's Embedding wrappers ([HuggingFaceEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.huggingface.HuggingFaceEmbeddings.html) and [OpenAIEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.openai.OpenAIEmbeddings.html)) to easily load the models and embed our document chunks.\n",
    "\n",
    "**Note**: embeddings aren't the only way to determine the more relevant chunks. We could also use an LLM to decide! However, because LLMs are much larger than these embedding models and have maximum context lengths, it's better to use embeddings to retrieve the top k chunks. And then we could use LLMs on the fewer k chunks to determine the <k chunks to use as the context to answer our query. We could also use reranking (ex. [Cohere Rerank](https://txt.cohere.com/rerank/)) to further identify the most relevant chunks to use. We could also combine embeddings with traditional information retrieval methods such as keyword matching, which could be useful for matching for unique tokens that may potentially be lost when embedding subtokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c066a-100b-4f09-98ba-e79a93cd4302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding_model(embedding_model_name, model_kwargs, encode_kwargs):\n",
    "    if embedding_model_name == \"text-embedding-ada-002\":\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            model=embedding_model_name,\n",
    "            openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "            openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    else:\n",
    "        embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model_name,  # also works with model_path\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs)\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b6a5a3-cd2d-4987-838a-be13e9553080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmbedChunks:\n",
    "    def __init__(self, model_name):\n",
    "        self.embedding_model = get_embedding_model(\n",
    "            embedding_model_name=model_name,\n",
    "            model_kwargs={\"device\": \"cuda\"},\n",
    "            encode_kwargs={\"device\": \"cuda\", \"batch_size\": 100})\n",
    "    def __call__(self, batch):\n",
    "        embeddings = self.embedding_model.embed_documents(batch[\"text\"])\n",
    "        return {\"text\": batch[\"text\"], \"source\": batch[\"source\"], \"embeddings\": embeddings}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e39c99a-5f33-4052-ba0e-cbc69d104ec1",
   "metadata": {},
   "source": [
    "Here we're able to embed our chunks at scale by using [map_batches](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html). All we had to do was define the `batch_size` and the compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9715a01e-dc67-4342-a0cb-30e770852097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embed chunks\n",
    "embedding_model_name = \"thenlper/gte-base\"\n",
    "embedded_chunks = chunks_ds.map_batches(\n",
    "    EmbedChunks,\n",
    "    fn_constructor_kwargs={\"model_name\": embedding_model_name},\n",
    "    batch_size=100, \n",
    "    num_gpus=1,\n",
    "    compute=ActorPoolStrategy(size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dffa1f-19a3-4411-af3f-b161a47ee164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample an embedding\n",
    "sample = embedded_chunks.take(1)\n",
    "print (\"embedding size:\", len(sample[0][\"embeddings\"]))\n",
    "print (sample[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09187588-e1dc-44c5-b88b-cc8ebe3f9c48",
   "metadata": {},
   "source": [
    "## Index data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9f53f1-2720-4a47-8b2f-1b206c38c425",
   "metadata": {},
   "source": [
    "Now that we have our embedded chunks, we need to index (store) them somewhere so that we can retrieve them quickly for inference. While there are many popular vector database options, we're going to use [Postgres with pgvector](https://github.com/pgvector/pgvector) for it's simplificty and performance. We'll create a table (`document`) and write the (`text`, `source`, `embedding`) triplets for each embedded chunk we have.\n",
    "\n",
    "<img width=\"700\" src=\"https://images.ctfassets.net/xjan103pcp94/3z1ryYkOtUjj6N1IuavJPf/ae60dc4a10c94e2cc928c38701befb51/image2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21480e47-4a17-49d8-b4e7-c301e8040a69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StoreResults:\n",
    "    def __call__(self, batch):\n",
    "        with psycopg.connect(os.environ[\"DB_CONNECTION_STRING\"]) as conn:\n",
    "            register_vector(conn)\n",
    "            with conn.cursor() as cur:\n",
    "                for text, source, embedding in zip(batch[\"text\"], batch[\"source\"], batch[\"embeddings\"]):\n",
    "                    cur.execute(\"INSERT INTO document (text, source, embedding) VALUES (%s, %s, %s)\", (text, source, embedding,),)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f119d-535d-4548-bb83-177e3bcefeed",
   "metadata": {},
   "source": [
    "And once again, we can use Ray Dataâ€™s [map_batches](https://docs.ray.io/en/latest/data/api/doc/ray.data.Dataset.map_batches.html) to perform this indexing in parallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4582d9-40ba-4a94-81ac-259b3851f837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Index data\n",
    "embedded_chunks.map_batches(\n",
    "    StoreResults,\n",
    "    batch_size=128,\n",
    "    num_cpus=1,\n",
    "    compute=ActorPoolStrategy(size=6),\n",
    ").materialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70046c8-efae-43f3-a82e-02884cf7e529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"SQL_DUMP_FP\"] = f\"/mnt/shared_storage/ray-assistant-data/{embedding_model_name.split('/')[-1]}_{chunk_size}_{chunk_overlap}.sql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40925cd6-41e8-4651-9692-aeb399b68af6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Dump the Postgres database to $SQL_DUMP_FP\n",
    "echo $SQL_DUMP_FP\n",
    "sudo -u postgres pg_dump -c > $SQL_DUMP_FP\n",
    "\n",
    "# You don't need to generate the embeddings again if you have $SQL_DUMP_FP (gte-base_300_50.sql in this example).\n",
    "# You can initialize Postgres with `sudo -u postgres psql -f $SQL_DUMP_FP`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

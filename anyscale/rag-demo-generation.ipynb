{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2729ed-3a3f-4dec-8850-53cb25e5dbef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import time\n",
    "import os\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import psycopg\n",
    "import json\n",
    "from pgvector.psycopg import register_vector\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7719d67-e177-4ddd-acf0-d10c57b45b82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prerequisites:\n",
    "# 1. Have a running Postgres service that already contains the Ray document embeddings.\n",
    "# 2. Create a `.env` file in the same directory as this notebook to initialize the `ANYSCALE_API_BASE` and `ANYSCALE_API_KEY` variables for connecting to Anyscale Endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a98f5d-3f5e-43fb-a1d5-229b678d9080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load environment variables from .env file in the same directory.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a4aa5f-ad79-4a24-bbb1-591a84196079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embedding dimensions\n",
    "EMBEDDING_DIMENSIONS = {\n",
    "    \"thenlper/gte-base\": 768,\n",
    "    \"thenlper/gte-large\": 1024,\n",
    "    \"BAAI/bge-large-en\": 1024,\n",
    "    \"text-embedding-ada-002\": 1536,\n",
    "    \"gte-large-fine-tuned\": 1024,\n",
    "}\n",
    "\n",
    "# Maximum context lengths\n",
    "MAX_CONTEXT_LENGTHS = {\n",
    "    \"gpt-4\": 8192,\n",
    "    \"gpt-3.5-turbo\": 4096,\n",
    "    \"gpt-3.5-turbo-16k\": 16384,\n",
    "    \"gpt-4-1106-preview\": 128000,\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\": 4096,\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\": 4096,\n",
    "    \"meta-llama/Llama-2-70b-chat-hf\": 4096,\n",
    "    \"codellama/CodeLlama-34b-Instruct-hf\": 16384,\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.1\": 65536,\n",
    "    \"mistralai/Mixtral-8x7B-Instruct-v0.1\": 32768,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6be6fb-60b8-4ce8-b0c4-2588b1612842",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_num_tokens(text):\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493f84b3-a74b-4995-9def-5c81e136008f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trim(text, max_context_length):\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return enc.decode(enc.encode(text)[:max_context_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b613de93-0b7c-433d-be89-2dcf9029269b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_client(llm):\n",
    "    if llm.startswith(\"gpt\"):\n",
    "        base_url = os.environ[\"OPENAI_API_BASE\"]\n",
    "        api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    else:\n",
    "        base_url = os.environ[\"ANYSCALE_API_BASE\"]\n",
    "        api_key = os.environ[\"ANYSCALE_API_KEY\"]\n",
    "    client = openai.OpenAI(base_url=base_url, api_key=api_key)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee84b123-7805-4d87-bcbe-d7020f1b6f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def semantic_search(query, embedding_model, k):\n",
    "    embedding = np.array(embedding_model.embed_query(query))\n",
    "    with psycopg.connect(\n",
    "        dbname=\"postgres\",\n",
    "        user=\"postgres\",\n",
    "        host=\"localhost\",\n",
    "        password=\"postgres\"\n",
    "    ) as conn:\n",
    "        register_vector(conn)\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(\"SELECT * FROM document ORDER BY embedding <=> %s LIMIT %s\", (embedding, k),)\n",
    "            rows = cur.fetchall()\n",
    "            semantic_context = [{\"id\": row[0], \"text\": row[1], \"source\": row[2]} for row in rows]\n",
    "    return semantic_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fd384c-a70c-4a5a-82d9-d7f682610f13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def response_stream(chat_completion):\n",
    "    for chunk in chat_completion:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content is not None:\n",
    "            yield content\n",
    "\n",
    "\n",
    "def prepare_response(chat_completion, stream):\n",
    "    if stream:\n",
    "        return response_stream(chat_completion)\n",
    "    else:\n",
    "        return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beed6c73-2391-446c-99e8-e2dc60db8b08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response(\n",
    "    llm, temperature=0.0, stream=True,\n",
    "    system_content=\"\", assistant_content=\"\", user_content=\"\", \n",
    "    max_retries=1, retry_interval=60):\n",
    "    \"\"\"Generate response from an LLM.\"\"\"\n",
    "    retry_count = 0\n",
    "    client = get_client(llm=llm)\n",
    "    messages = [{\"role\": role, \"content\": content} for role, content in [\n",
    "        (\"system\", system_content), \n",
    "        (\"assistant\", assistant_content), \n",
    "        (\"user\", user_content)] if content]\n",
    "    while retry_count <= max_retries:\n",
    "        try:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                model=llm,\n",
    "                temperature=temperature,\n",
    "                stream=stream,\n",
    "                messages=messages,\n",
    "            )\n",
    "            return prepare_response(chat_completion, stream)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "            time.sleep(retry_interval)  # default is per-minute rate limits\n",
    "            retry_count += 1\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68afb06a-eae0-4efa-827e-52858365ceca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embedding_model(embedding_model_name, model_kwargs, encode_kwargs):\n",
    "    if embedding_model_name == \"text-embedding-ada-002\":\n",
    "        embedding_model = OpenAIEmbeddings(\n",
    "            model=embedding_model_name,\n",
    "            openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
    "            openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    else:\n",
    "        embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=embedding_model_name,  # also works with model_path\n",
    "            model_kwargs=model_kwargs,\n",
    "            encode_kwargs=encode_kwargs)\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2048f6-cdcf-4741-aa11-90721f780684",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QueryAgent:\n",
    "    def __init__(self, embedding_model_name=\"thenlper/gte-base\",\n",
    "                 llm=\"meta-llama/Llama-2-70b-chat-hf\", temperature=0.0, \n",
    "                 max_context_length=4096, system_content=\"\", assistant_content=\"\"):\n",
    "        \n",
    "        # Embedding model\n",
    "        self.embedding_model = get_embedding_model(\n",
    "            embedding_model_name=embedding_model_name, \n",
    "            model_kwargs={\"device\": \"cuda\"}, \n",
    "            encode_kwargs={\"device\": \"cuda\", \"batch_size\": 100})\n",
    "        \n",
    "        # Context length (restrict input length to 50% of total context length)\n",
    "        max_context_length = int(0.5*max_context_length)\n",
    "        \n",
    "        # LLM\n",
    "        self.llm = llm\n",
    "        self.temperature = temperature\n",
    "        self.context_length = max_context_length - get_num_tokens(system_content + assistant_content)\n",
    "        self.system_content = system_content\n",
    "        self.assistant_content = assistant_content\n",
    "\n",
    "    def __call__(self, query, num_chunks=5, stream=True):\n",
    "        # Get sources and context\n",
    "        context_results = semantic_search(\n",
    "            query=query, \n",
    "            embedding_model=self.embedding_model, \n",
    "            k=num_chunks)\n",
    "            \n",
    "        # Generate response\n",
    "        context = [item[\"text\"] for item in context_results]\n",
    "        sources = [item[\"source\"] for item in context_results]\n",
    "        user_content = f\"query: {query}, context: {context}\"\n",
    "        answer = generate_response(\n",
    "            llm=self.llm,\n",
    "            temperature=self.temperature,\n",
    "            stream=stream,\n",
    "            system_content=self.system_content,\n",
    "            assistant_content=self.assistant_content,\n",
    "            user_content=trim(user_content, self.context_length))\n",
    "\n",
    "        # Result\n",
    "        result = {\n",
    "            \"question\": query,\n",
    "            \"sources\": sources,\n",
    "            \"answer\": answer,\n",
    "            \"llm\": self.llm,\n",
    "        }\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574ef1e-3007-45f6-842f-9cc43ed77c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_name = \"thenlper/gte-base\"\n",
    "llm = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "query = \"What is the default batch size for map_batches?\"\n",
    "\n",
    "agent = QueryAgent(\n",
    "    embedding_model_name=embedding_model_name,\n",
    "    llm=llm,\n",
    "    max_context_length=MAX_CONTEXT_LENGTHS[llm],\n",
    "    system_content=system_content)\n",
    "result = agent(query=query, stream=False)\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99da4ab-9c89-4ce5-839a-890b3315ea58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
